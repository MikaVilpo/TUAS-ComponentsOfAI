{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e19b281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\JanTilles\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2, EfficientNetB0\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Model\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "# Add task-specific layers with improved Dropout\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3), \n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5), \n",
    "    layers.Dense(4, activation='softmax')  # Adjust number of classes accordingly\n",
    "])\n",
    "\n",
    "# Implement Learning Rate Decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model with improved learning rate tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='sparse_categorical_crossentropy',  \n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845c38e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3919 files belonging to 4 classes.\n",
      "Found 395 files belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image, label):\n",
    "    image = tf.image.resize(image, (224, 224)) / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Load datasets using tf.data pipeline\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/train',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ").map(preprocess_image).cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    'dataset/val',\n",
    "    image_size=(224, 224),\n",
    "    batch_size=32\n",
    ").map(preprocess_image).cache().prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f52eb15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.6310 - loss: 0.9443\n",
      "Epoch 1: val_loss improved from inf to 0.47302, saving model to best_model.keras\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 324ms/step - accuracy: 0.6318 - loss: 0.9422 - val_accuracy: 0.8532 - val_loss: 0.4730\n",
      "Epoch 2/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.8306 - loss: 0.4524\n",
      "Epoch 2: val_loss improved from 0.47302 to 0.43870, saving model to best_model.keras\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 309ms/step - accuracy: 0.8307 - loss: 0.4524 - val_accuracy: 0.8608 - val_loss: 0.4387\n",
      "Epoch 3/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 279ms/step - accuracy: 0.8504 - loss: 0.4045\n",
      "Epoch 3: val_loss improved from 0.43870 to 0.42679, saving model to best_model.keras\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 312ms/step - accuracy: 0.8504 - loss: 0.4045 - val_accuracy: 0.8759 - val_loss: 0.4268\n",
      "Epoch 4/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.8736 - loss: 0.3510\n",
      "Epoch 4: val_loss did not improve from 0.42679\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 323ms/step - accuracy: 0.8735 - loss: 0.3511 - val_accuracy: 0.8608 - val_loss: 0.4324\n",
      "Epoch 5/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 285ms/step - accuracy: 0.8782 - loss: 0.3297\n",
      "Epoch 5: val_loss did not improve from 0.42679\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 313ms/step - accuracy: 0.8781 - loss: 0.3298 - val_accuracy: 0.8861 - val_loss: 0.4328\n",
      "Epoch 6/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step - accuracy: 0.8844 - loss: 0.3113\n",
      "Epoch 6: val_loss improved from 0.42679 to 0.42391, saving model to best_model.keras\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 313ms/step - accuracy: 0.8845 - loss: 0.3112 - val_accuracy: 0.8810 - val_loss: 0.4239\n",
      "Epoch 7/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 298ms/step - accuracy: 0.8988 - loss: 0.2934\n",
      "Epoch 7: val_loss improved from 0.42391 to 0.41809, saving model to best_model.keras\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 332ms/step - accuracy: 0.8987 - loss: 0.2935 - val_accuracy: 0.8532 - val_loss: 0.4181\n",
      "Epoch 8/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 286ms/step - accuracy: 0.8960 - loss: 0.2845\n",
      "Epoch 8: val_loss did not improve from 0.41809\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 316ms/step - accuracy: 0.8960 - loss: 0.2846 - val_accuracy: 0.8759 - val_loss: 0.4288\n",
      "Epoch 9/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.9046 - loss: 0.2582\n",
      "Epoch 9: val_loss improved from 0.41809 to 0.41258, saving model to best_model.keras\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 325ms/step - accuracy: 0.9046 - loss: 0.2583 - val_accuracy: 0.8684 - val_loss: 0.4126\n",
      "Epoch 10/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 295ms/step - accuracy: 0.9092 - loss: 0.2510\n",
      "Epoch 10: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 325ms/step - accuracy: 0.9092 - loss: 0.2510 - val_accuracy: 0.8633 - val_loss: 0.4632\n",
      "Epoch 11/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.9111 - loss: 0.2437\n",
      "Epoch 11: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 319ms/step - accuracy: 0.9111 - loss: 0.2437 - val_accuracy: 0.8734 - val_loss: 0.4348\n",
      "Epoch 12/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 289ms/step - accuracy: 0.9162 - loss: 0.2255\n",
      "Epoch 12: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 319ms/step - accuracy: 0.9163 - loss: 0.2255 - val_accuracy: 0.8759 - val_loss: 0.4392\n",
      "Epoch 1/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.7832 - loss: 0.5915\n",
      "Epoch 1: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 2s/step - accuracy: 0.7836 - loss: 0.5906 - val_accuracy: 0.8278 - val_loss: 0.9058\n",
      "Epoch 2/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9623 - loss: 0.1330\n",
      "Epoch 2: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m153s\u001b[0m 1s/step - accuracy: 0.9623 - loss: 0.1330 - val_accuracy: 0.8810 - val_loss: 0.6812\n",
      "Epoch 3/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9828 - loss: 0.0617\n",
      "Epoch 3: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 1s/step - accuracy: 0.9828 - loss: 0.0617 - val_accuracy: 0.8152 - val_loss: 1.3304\n",
      "Epoch 4/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9955 - loss: 0.0194\n",
      "Epoch 4: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.9955 - loss: 0.0194 - val_accuracy: 0.7873 - val_loss: 1.4541\n",
      "Epoch 5/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.0090\n",
      "Epoch 5: val_loss did not improve from 0.41258\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m158s\u001b[0m 1s/step - accuracy: 0.9992 - loss: 0.0090 - val_accuracy: 0.8101 - val_loss: 1.4392\n"
     ]
    }
   ],
   "source": [
    "# Define callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "checkpoint_callback = ModelCheckpoint(\"best_model.keras\", save_best_only=True, monitor=\"val_loss\", verbose=1)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping, checkpoint_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune the entire model\n",
    "base_model.trainable = True  # Unfreeze the base model\n",
    "\n",
    "# Recompile model with a lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='sparse_categorical_crossentropy',  # Or categorical if using one-hot\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Fine-tune the model with a new EarlyStopping instance\n",
    "history_fine = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping, checkpoint_callback],  # New callbacks\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ed0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # You can choose different versions like yolov8s.pt, yolov8m.pt, etc.\n",
    "\n",
    "# Freeze the base model initially\n",
    "for param in model.model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Add task-specific layers (this part is more complex with YOLOv8 and typically involves modifying the model's head)\n",
    "# YOLOv8 is not directly compatible with Keras layers, so this step is different from typical Keras models\n",
    "\n",
    "# Implement Learning Rate Decay\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "# Compile the model with improved learning rate tuning\n",
    "# Note: YOLOv8 uses PyTorch, so you would use PyTorch optimizers and loss functions\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_schedule)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Example training loop (simplified)\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in dataloader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
