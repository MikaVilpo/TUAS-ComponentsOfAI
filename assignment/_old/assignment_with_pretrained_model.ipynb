{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f3bc04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import cv2\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8f4c91",
   "metadata": {},
   "source": [
    "### Step 1: Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c885bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Pretrained Model\n",
    "base_model = MobileNetV2(input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "base_model.trainable = False  # Freeze the base model initially\n",
    "\n",
    "# Add task-specific layers\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(4, activation='softmax')  # Replace 4 with the number of your classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a5256c",
   "metadata": {},
   "source": [
    "### Step 2: Prepare Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71672f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3919 images belonging to 4 classes.\n",
      "Found 395 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Set the base directory\n",
    "base_dir = os.getcwd()  # Get the current working directory\n",
    "\n",
    "# Construct the path to the dataset\n",
    "dataset_dir = os.path.join(base_dir, 'dataset')\n",
    "\n",
    "# Construct the path to the training directory\n",
    "train_dir = os.path.join(dataset_dir, 'train')\n",
    "\n",
    "# Construct the path to the validation directory\n",
    "val_dir = os.path.join(dataset_dir, 'val')\n",
    "\n",
    "# Data augmentation for training\n",
    "data_augmentation = ImageDataGenerator(\n",
    "    rescale=1.0 / 255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "# No augmentation for validation, just rescaling\n",
    "val_datagen = ImageDataGenerator(rescale=1.0 / 255.0)\n",
    "\n",
    "# Prepare generators\n",
    "train_generator = data_augmentation.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode=\"categorical\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1853c5cd",
   "metadata": {},
   "source": [
    "### Step 3: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "473b31c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 454ms/step - accuracy: 0.8113 - loss: 0.5115 - val_accuracy: 0.8430 - val_loss: 0.4426\n",
      "Epoch 2/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 480ms/step - accuracy: 0.8135 - loss: 0.4966 - val_accuracy: 0.8557 - val_loss: 0.4614\n",
      "Epoch 3/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 538ms/step - accuracy: 0.8289 - loss: 0.4476 - val_accuracy: 0.8633 - val_loss: 0.4555\n",
      "Epoch 4/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 503ms/step - accuracy: 0.8396 - loss: 0.4298 - val_accuracy: 0.8886 - val_loss: 0.4444\n",
      "Epoch 5/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 487ms/step - accuracy: 0.8463 - loss: 0.4073 - val_accuracy: 0.8785 - val_loss: 0.4193\n",
      "Epoch 6/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 497ms/step - accuracy: 0.8596 - loss: 0.3845 - val_accuracy: 0.8759 - val_loss: 0.4335\n",
      "Epoch 7/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 504ms/step - accuracy: 0.8569 - loss: 0.3879 - val_accuracy: 0.8658 - val_loss: 0.4540\n",
      "Epoch 8/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 513ms/step - accuracy: 0.8531 - loss: 0.3972 - val_accuracy: 0.8684 - val_loss: 0.4265\n",
      "Epoch 9/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 500ms/step - accuracy: 0.8610 - loss: 0.3786 - val_accuracy: 0.8709 - val_loss: 0.4304\n",
      "Epoch 10/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 487ms/step - accuracy: 0.8564 - loss: 0.3708 - val_accuracy: 0.8734 - val_loss: 0.4371\n",
      "Epoch 1/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m182s\u001b[0m 1s/step - accuracy: 0.7561 - loss: 0.6113 - val_accuracy: 0.7873 - val_loss: 1.0013\n",
      "Epoch 2/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m156s\u001b[0m 1s/step - accuracy: 0.8711 - loss: 0.3475 - val_accuracy: 0.8025 - val_loss: 0.9120\n",
      "Epoch 3/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.9093 - loss: 0.2585 - val_accuracy: 0.8405 - val_loss: 0.6235\n",
      "Epoch 4/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 1s/step - accuracy: 0.9198 - loss: 0.2358 - val_accuracy: 0.8608 - val_loss: 0.6809\n",
      "Epoch 5/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9315 - loss: 0.2028 - val_accuracy: 0.8709 - val_loss: 0.5234\n",
      "Epoch 6/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9484 - loss: 0.1585 - val_accuracy: 0.8911 - val_loss: 0.5359\n",
      "Epoch 7/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.9460 - loss: 0.1521 - val_accuracy: 0.8962 - val_loss: 0.5588\n",
      "Epoch 8/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 1s/step - accuracy: 0.9588 - loss: 0.1304 - val_accuracy: 0.8481 - val_loss: 0.6461\n",
      "Epoch 9/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.9588 - loss: 0.1179 - val_accuracy: 0.8937 - val_loss: 0.5924\n",
      "Epoch 10/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 1s/step - accuracy: 0.9665 - loss: 0.0991 - val_accuracy: 0.8861 - val_loss: 0.6127\n",
      "Epoch 11/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m161s\u001b[0m 1s/step - accuracy: 0.9643 - loss: 0.1147 - val_accuracy: 0.8709 - val_loss: 0.7143\n",
      "Epoch 12/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m170s\u001b[0m 1s/step - accuracy: 0.9678 - loss: 0.1176 - val_accuracy: 0.8734 - val_loss: 0.5549\n",
      "Epoch 13/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 1s/step - accuracy: 0.9634 - loss: 0.1046 - val_accuracy: 0.8759 - val_loss: 0.8222\n",
      "Epoch 14/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m172s\u001b[0m 1s/step - accuracy: 0.9734 - loss: 0.0814 - val_accuracy: 0.9063 - val_loss: 0.5619\n",
      "Epoch 15/15\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m160s\u001b[0m 1s/step - accuracy: 0.9678 - loss: 0.0854 - val_accuracy: 0.8987 - val_loss: 0.5125\n"
     ]
    }
   ],
   "source": [
    "# Train the model with early stopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train the new layers initially using train_generator and val_generator\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune the entire model\n",
    "base_model.trainable = True  # Unfreeze the base layers\n",
    "\n",
    "# Recompile with a lower learning rate\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the entire model\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=15,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "model.save('best_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b005a4b7",
   "metadata": {},
   "source": [
    "### Step 4: Real-Time Webcam Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8cb4830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to exit the video feed.\n",
      "Resources released, video window closed.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model = tf.keras.models.load_model('best_fine_tuned_model.keras')\n",
    "\n",
    "# Class labels for predictions\n",
    "class_labels = ['Headtop', 'Helmet', 'Hoodie', 'No headwear']\n",
    "\n",
    "# Open the webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 is the default camera\n",
    "\n",
    "# Define the target size for the images\n",
    "target_size = (224, 224)\n",
    "\n",
    "# Initialize a buffer for smoothing predictions\n",
    "predictions_buffer = collections.deque(maxlen=10)\n",
    "\n",
    "print(\"Press 'q' to exit the video feed.\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to capture video frame. Exiting...\")\n",
    "            break\n",
    "\n",
    "        # Preprocess the frame\n",
    "        resized_frame = cv2.resize(frame, target_size)  # Resize to match model input\n",
    "        img_array = np.expand_dims(resized_frame, axis=0) / 255.0  # Normalize and add batch dimension\n",
    "\n",
    "        # Make prediction\n",
    "        predictions = model.predict(img_array, verbose=0)\n",
    "        class_index = np.argmax(predictions[0])\n",
    "        prediction_label = class_labels[class_index]\n",
    "        confidence = predictions[0][class_index] * 100\n",
    "\n",
    "        # Add prediction to the buffer\n",
    "        predictions_buffer.append(class_index)\n",
    "\n",
    "        # Smooth predictions using majority voting\n",
    "        smoothed_prediction = max(set(predictions_buffer), key=predictions_buffer.count)\n",
    "        smoothed_label = class_labels[smoothed_prediction]\n",
    "\n",
    "        # Display the prediction on the frame\n",
    "        cv2.putText(frame, f\"{smoothed_label} ({confidence:.2f}%)\", \n",
    "                    (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (75, 75, 75), 2)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow('Hat Detection', frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProgram interrupted by the user. Exiting...\")\n",
    "\n",
    "finally:\n",
    "    # Release the webcam and close windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"Resources released, video window closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
